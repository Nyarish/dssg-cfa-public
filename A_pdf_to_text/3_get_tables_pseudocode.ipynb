{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Process\n",
    "\n",
    "If you *can* use form recognizer Layout API for all of the Gazettes, this is preferable. \n",
    "\n",
    "If you can't use form recognizer Layout API for all of the Gazettes: \n",
    "* Get list of all gazette pages that have tables in them, using the column number estimator.<sup>1</sup> If a page is estimated to have \"None\" or more than 2 columns, it's likely a table. \n",
    "* Get the full PDF of those gazettes\n",
    "* Filter for the page(s) (remember that PDF Reader Objects start indexing at zero) \n",
    "* Send the PDF data for the filtered pages to the form recognizer API\n",
    "\n",
    "<sup>1</sup>There are limitations with the column estimation strategy. In particular, if a page contains one small table (e.g., 1-2 rows) and is otherwise a two-column page, the column estimator will not identify it as containing tables. \n",
    "\n",
    "A possible improvement would be to filter pages for Notices of interest that are anticipated to contain tables. For example, a project interested in the Land Act could find pages containing a Land Act announcement. \n",
    "\n",
    "Once you have the tables: \n",
    "* Knit the tables together using the information provided: \n",
    "    + If `resp_json` is json format of response object, then `resp_json['analyzeResult']['pageResults'][0]['tables']` will give the tables on page 0\n",
    "    + Gives rowIndex & columnIndex, + bounding box for the text, + the text\n",
    "    \n",
    "Unfortunately, we did not have time to implement this process fully. We hope that the below starting point, as well as the explanation of the Form Recognizer Layout API output described in the `additional_walkthroughs` folder, will be a starting point for someone else to implement this in a straightforward way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from helpers import json_extraction as je\n",
    "from helpers import write_urls as wu \n",
    "\n",
    "ROUTETOROOTDIR = '/home/dssg-cfa/notebooks/dssg-cfa-public/'\n",
    "IMPORTSCRIPTSDIR = ROUTETOROOTDIR + \"util/py_files\"\n",
    "os.chdir(IMPORTSCRIPTSDIR)\n",
    "import orderingText\n",
    "\n",
    "ke_gazettes = \"/home/dssg-cfa/ke-gazettes/\"\n",
    "filenames = [f for f in os.listdir(ke_gazettes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gazette-ke-vol-cx-no-100-dated-19-december-2008-special': []},\n",
       " {'gazette-ke-vol-cvii-no-34-dated-13-may-2005': [0, 35]},\n",
       " {'gazette-ke-vol-cxiv-no-45-dated-23-may-2012-special': []},\n",
       " {'gazette-ke-vol-cxviii-no-163-dated-23-december-2016': [1,\n",
       "   2,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   31]},\n",
       " {'gazette-ke-vol-cxix-no-94-dated-10-july-2017-special': [1]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pseudocode to build off of:\n",
    "\n",
    "likely_tables = []\n",
    "for fn in filenames[:5]: \n",
    "    with open(ke_gazettes + fn) as f:\n",
    "        data_json = json.load(f)\n",
    "    content = data_json['analyzeResult']['readResults']\n",
    "    pg_lst = []\n",
    "    for i in range(len(content)):\n",
    "        page_lines = content[i]['lines']\n",
    "        num = orderingText.getNumCols(page_lines)\n",
    "        if (num != None and num > 2) or (num == None):\n",
    "            if i == 0 and len(content) == 1: \n",
    "                continue # skip table of contents \n",
    "            pg_lst.append(i)\n",
    "            \n",
    "    # POSSIBLE PSEUDOCODE: \n",
    "    # access source database metadata, using our map from the DSSG filenames\n",
    "    # use this metadata to access a URL  \n",
    "    # call form recognizer API on the page, passing it the URL and the pg_lst (indices)\n",
    "        # json_extraction.call_form_rec_layout_api() will be helpful for this\n",
    "    # append results of form recognizer API to the JSON, with page numbers attached \n",
    "    # re-save the JSON under the same filename -- now including Form Recognizer results\n",
    "    likely_tables.append({fn: pg_lst})\n",
    "        \n",
    "likely_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
