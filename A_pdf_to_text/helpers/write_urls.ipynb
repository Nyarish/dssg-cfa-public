{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: Write URLs\n",
    "Created July 2020\n",
    "-----------------\n",
    "For getting and writing Kenya Gazette URLs to provided filepath. \n",
    "Supports: \n",
    "- Connected Africa (use argument: connected_africa)\n",
    "- Gazeti.Africa (use argument: gazeti)\n",
    "-----------------\n",
    "The URLs written are the final destination URLs, which point directly to\n",
    "the gazette PDFs. \n",
    "-----------------\n",
    "Please see the \"Getting Data Walkthrough\" file in the \"additional documentation\"\n",
    "folder associated with step A (PDF to text) for more detailed explanation of\n",
    "each of these outputs.\n",
    "-----------------\n",
    "Note that there is repeated code here; the functions for getting metadata were\n",
    "written later in the development process. \n",
    "'''\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "'''\n",
    "Before downloading Gazeti files, you will need to download the URLs from\n",
    "the Gazeti website, save them, and open the file wherever they are saved. \n",
    "Change this to reflect where you have saved the Gazeti files, and please\n",
    "see the \"Getting Data Walkthrough\" for more details.\n",
    "'''\n",
    "\n",
    "gazeti_url_file = \"/home/dssg-cfa/final_dest_urls/export.csv\"\n",
    "\n",
    "except_msg = \"Please export gazette urls from gazeti.africa website.\\n \\\n",
    "        Link to downloading Excel file for all Kenya Gazettes:\\n \\\n",
    "        https://gazeti.africa/api/1/query/export?filter:collection_id=1\\n \\\n",
    "        Convert to CSV, then upload as \\'export.csv\\'.\\n \\\n",
    "        to /home/dssg-cfa/final_dest_urls/export.csv \\n \\\n",
    "        Possible improvement: automate this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Helpers for getting data from the Gazeti website. \n",
    "'''\n",
    "\n",
    "def conn_afr_api_call():\n",
    "    '''  \n",
    "    This calls the Connected Africa API and returns metadata for all gazettes stored\n",
    "    in the database. The metadata includes links, publication date, and other \n",
    "    identifying information. \n",
    "    '''\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': 'ApiKey c7ce69ddd9764dd095f8b2a3e157715f',\n",
    "    }\n",
    "\n",
    "\n",
    "    params = (\n",
    "        ('facet', 'collection_id'),\n",
    "        ('facet_size:collection_id', '10'),\n",
    "        ('facet_total:collection_id', 'true'),\n",
    "        ('filter:collection_id', '18'),\n",
    "        ('filter:schema', 'Pages'),\n",
    "        ('filter:schemata', 'Thing'),\n",
    "        ('highlight', 'true'),\n",
    "        ('limit', '10000'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://data.connectedafrica.net/api/2/entities', headers=headers, params=params)\n",
    "\n",
    "    data = response.text\n",
    "\n",
    "    data_json = json.loads(data)\n",
    "    \n",
    "    return data_json\n",
    "\n",
    "\n",
    "\n",
    "def get_img_urls_conn_af():\n",
    "    '''\n",
    "    Returns list of tuples with the format:\n",
    "    tuple[0] = image url (which redirects to final destination URL)\n",
    "    tuple[1] = year of publication\n",
    "    These are from the Connected Africa website.\n",
    "    Note that the \"image URL\" will not work with the Read API; the URL it redirects to\n",
    "    is needed. \n",
    "    '''\n",
    "    \n",
    "    # Database #1: Connected Africa, accessed through API call\n",
    "    print(\"Getting image redirect URLs from Connected Africa website...\")\n",
    "    \n",
    "    data_json_ca = conn_afr_api_call()\n",
    "\n",
    "    img_urls = []\n",
    "    for result in data_json_ca['results']: \n",
    "        dated_str = result['properties']['publishedAt'][0]\n",
    "        year = int(dated_str[:4])\n",
    "        url = result['links']['file']\n",
    "        \n",
    "        if url in img_urls:\n",
    "            print(\"duplicate\")\n",
    "            print(url)\n",
    "        \n",
    "        img_urls.append((url, year))\n",
    "\n",
    "    print(\"Done.\\n\")\n",
    "    return img_urls\n",
    "        \n",
    "                     \n",
    "def get_ca_urls_metadata(yr_start = 0, yr_end = 7000): \n",
    "    '''\n",
    "    Writes a JSON to filepath_out with gazette URLs from Connected Africa database\n",
    "    and identifying information.\n",
    "    \n",
    "    For each gazette, this will look like a dictionary with the following keys: \n",
    "    - year: year of publication\n",
    "    - fileNameDirectFromDB: filename, as it is written in the source database\n",
    "    - dest_url: this is a temporary URL that points directly to the file. \n",
    "    - checksums: a hash of the content; unique for content\n",
    "    - docid: the document ID; unique id for files in the database \n",
    "    - dest_url: a temporary URL (this will refresh every few days) pointing to the gazette PDF \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data_json_ca = conn_afr_api_call()\n",
    "    \n",
    "    final_list = []\n",
    "    count = 0\n",
    "    print(\"Getting destination URLs and metadata...\")\n",
    "    \n",
    "    for result in data_json_ca['results']:\n",
    "        one_gazette = {}\n",
    "        \n",
    "        year = int(result['properties']['publishedAt'][0][:4])\n",
    "        if year < yr_start or year > yr_end: \n",
    "            continue \n",
    "        \n",
    "        one_gazette['year'] = year\n",
    "        one_gazette['fileNameDirectFromDB'] = result['name']\n",
    "        one_gazette['checksums'] = result['checksums']\n",
    "        one_gazette['docid'] = result['id']\n",
    "        url = result['links']['file']\n",
    "        one_gazette['dest_url'] = requests.get(url, allow_redirects=False).headers['Location']\n",
    "        \n",
    "        final_list.append(one_gazette)\n",
    "                \n",
    "        count += 1\n",
    "        if (count % 100 == 0 and count < 500) or (count % 500 == 0):\n",
    "            print(\"Got info for \" + str(count) + \" gazettes\")\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Helper functions for getting data from Gazeti file. \n",
    "'''\n",
    "\n",
    "def get_img_urls_gazeti():\n",
    "    '''\n",
    "    Returns dictionary with the format:\n",
    "    tuple[0] = image url (which redirects to final destination URL)\n",
    "    tuple[1] = year of publication\n",
    "    These are from the gazeti.africa website\n",
    "    '''\n",
    "\n",
    "    print(\"Getting image redirect URLs from file \" + gazeti_url_file + \"...\")\n",
    "    if not os.path.exists(gazeti_url_file):\n",
    "        raise Exception(\"Error: \" + gazeti_url_file + \" does not exist. \\n\" + except_msg)\n",
    "  \n",
    "    img_urls = []\n",
    "    reader = csv.DictReader(open(gazeti_url_file, encoding='utf-8-sig')) \n",
    "    for line in reader:\n",
    "        if line['File url'] in img_urls:\n",
    "            print(\"duplicate\")\n",
    "            print(line['File url'])\n",
    "\n",
    "        title = line['Title']\n",
    "        year = int(title[-4:])\n",
    "        img_urls.append((line['File url'], year))\n",
    "\n",
    "    print(\"Done\\n\")\n",
    "    return img_urls \n",
    "\n",
    "\n",
    "def get_gazeti_urls_metadata(yr_start = 0, yr_end = 7000):\n",
    "    '''\n",
    "    Writes a JSON to filepath_out with gazette URLs and identifying information\n",
    "    about it and how it is stored in the source database. \n",
    "    \n",
    "    For each gazette, this will look like a dictionary with the following keys:\n",
    "    \n",
    "    - year: year of publication\n",
    "    - fileNameDirectFromDB: filename, as it is written in the source database\n",
    "    - file_url: this is a permanent URL that redirects to the file. \n",
    "    it will be of the form: https://gazeti.africa/api/1/documents/[file_num]/file\n",
    "    - dest_url: this is a temporary URL that points directly to the file. \n",
    "    - file_num: the unique number of the file in the Gazeti database. \n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(gazeti_url_file):\n",
    "        raise Exception(\"Error: \" + gazeti_url_file + \" does not exist. \\n\" + except_msg)\n",
    "  \n",
    "    \n",
    "    final_list = []\n",
    "    count = 0\n",
    "    reader = csv.DictReader(open(img_url_file, encoding='utf-8-sig')) \n",
    "    print(\"Getting destination URLs and metadata...\")\n",
    "\n",
    "    for line in reader:\n",
    "        one_gazette = {}\n",
    "        \n",
    "        title = line['Title']\n",
    "        year = int(title[-4:])\n",
    "        if year < yr_start or year > yr_end: \n",
    "            continue \n",
    "\n",
    "        one_gazette['year'] = year\n",
    "        one_gazette['fileNameDirectFromDB'] = line['File name'].replace('.pdf', '')\n",
    "        url = line['File url']\n",
    "        one_gazette['file_url'] = url\n",
    "        file_num = re.search('documents/\\d+', one_gazette['file_url']).group(0).replace(\"documents/\", \"\")\n",
    "        one_gazette['file_num'] = \"num-\" + file_num\n",
    "        one_gazette['dest_url'] = requests.get(url, allow_redirects=False).headers['Location']\n",
    "        \n",
    "        final_list.append(one_gazette)\n",
    "        \n",
    "        count += 1\n",
    "        if (count % 100 == 0 and count < 500) or (count % 500 == 0):\n",
    "            print(\"Got info for \" + str(count) + \" gazettes\")\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. General function to write destination URLs in bulk. \n",
    "'''\n",
    "\n",
    "def write_dest_urls(source, filepath_out, metadata = False, yr_start = None, yr_end = None):\n",
    "    '''    \n",
    "    - Source: \"connected_africa\" or \"gazeti\"\n",
    "    - filepath_out: the filepath wishing to save to\n",
    "    - \"metadata\": specify whether to write out just the URLs or the \n",
    "    URLs and identifying metadata about documents. (default is False.)\n",
    "    - yr_start & yr_end: if filtering for dates; inclusive\n",
    "    '''\n",
    "    \n",
    "    # error checking: valid year range, valid filepath, valid source\n",
    "    if yr_start and yr_end and yr_start > yr_end:\n",
    "        raise Exception(\"Start year should be before end year.\")\n",
    "   \n",
    "    f = open(filepath_out, 'w') \n",
    "    f.close()\n",
    "    \n",
    "    if source != \"connected_africa\" and source != \"gazeti\": \n",
    "        msg = \"Invalid Source.\\nPlease enter either \\'connected_africa\\' or \\'gazeti\\'\"\n",
    "        raise Exception(msg)\n",
    "    \n",
    "    # calls metadata functions, which filter by year and save JSONs to the file\n",
    "    if metadata: \n",
    "        if source == \"connected_africa\": \n",
    "            final_list = get_ca_urls_metadata(yr_start, yr_end)\n",
    "        elif source == \"gazeti\" and metadata:\n",
    "            final_list = get_gazeti_urls_metadata(yr_start, yr_end)\n",
    "        with open(filepath_out, 'w') as f: \n",
    "            json.dump(final_list, f)\n",
    "        print(\"Done: \" + str(count) + \" urls saved to \" + filepath_out)\n",
    "        return\n",
    "\n",
    "    \n",
    "    # calls functions to just get URLs \n",
    "    if source == \"connected_africa\":\n",
    "        img_urls = get_img_urls_conn_af()\n",
    "    elif source == \"gazeti\":\n",
    "        img_urls = get_img_urls_gazeti()\n",
    "           \n",
    "    if yr_start: \n",
    "        img_urls = list(filter(lambda t: t[1] >= yr_start, img_urls))\n",
    "    if yr_end: \n",
    "        img_urls = list(filter(lambda t: t[1] <= yr_end, img_urls))\n",
    "    \n",
    "    img_urls = list(set([t[0] for t in img_urls]))\n",
    "    \n",
    "    final_dest_list = []\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Preparing to fetch \" + str(len(img_urls)) + \" final destination URLs\\n\")\n",
    "    for url in img_urls: \n",
    "        final_dest_list.append(requests.get(url, allow_redirects=False).headers['Location'])\n",
    "        count += 1\n",
    "        if (count % 100 == 0 and count < 500) or (count % 500 == 0):\n",
    "            print(\"Got \" + str(count) + \" final destination URLs\")\n",
    "    \n",
    "    final_dest_list = list(set(final_dest_list))\n",
    "    \n",
    "    print(\"\\nSuccess! Saving final destination URLs to \" + filepath_out)\n",
    "    print(\"Note that \" + str(len(img_urls) - len(final_dest_list)) + \" were duplicates\")\n",
    "    \n",
    "    with open(filepath_out, 'w') as f:\n",
    "        for url in final_dest_list:\n",
    "            f.write('%s\\n' % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
