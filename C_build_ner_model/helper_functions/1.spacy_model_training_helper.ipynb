{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "#\n",
    "# This file contains helper functions to train a modified spaCy NER model  \n",
    "# using training segments extracted from the Kenya gazettes.\n",
    "#\n",
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep these in a separate chunk otherwise you get error\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModifiedNERModel(training_data, model=None, new_model_name=\"modified_ner_model_gazettes\", output_dir=None, n_iter=10):\n",
    "    \"\"\"\n",
    "    A function to train a modified NER model using custom training data. This function was copied and modified from https://spacy.io/usage/training\n",
    "    \n",
    "    Arguments:\n",
    "    training_data -- list of training samples. Each training sample is a list in the form [TEXT, {'entities': (START_CHAR, END_CHAR, LABEL)}]\n",
    "    model -- a string specifying the base model. If None, the model will be constructed from scratch.\n",
    "    new_model_name -- a string specifying the desired new model name\n",
    "    output_dir -- a string to the directory where the new model should be stored\n",
    "    n_iter -- an int specifying the number of iterations for model training\n",
    "    \n",
    "    Output:\n",
    "    'Model Trained and Saved.' -- a string confirming that the model training is completed. \n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model, if there is any\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "        \n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add new entity label to entity recognizer    \n",
    "    for i in all_labels:\n",
    "        ner.add_label(i)\n",
    "    \n",
    "    \n",
    "    if model is None:\n",
    "    #if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    " \n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        \n",
    "    \n",
    "    return \"Model Trained and Saved.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOverlapsAndBadEntries(tupleTags):\n",
    "    \"\"\"\n",
    "    A function to remove tuples that represent entity tags so that no two tuples overlap\n",
    "    concerning the characters they use in the original text.\n",
    "    \n",
    "    Arguments:\n",
    "    tupleTags -- a tuple of entities of entities of the form (START_CHAR, END_CHAR, LABEL)\n",
    "    \n",
    "    Returns:\n",
    "    tupleTags -- a tuple of entities of entities of the form (START_CHAR, END_CHAR, LABEL) with overlapping tags removed    \n",
    "    \"\"\"\n",
    "    \n",
    "    charsUsedOverall = set()\n",
    "    overlapTagNums = []\n",
    "    \n",
    "    #find tag num which overlaps with previous tags\n",
    "    for tagNum in range(len(tupleTags)):\n",
    "        tag = tupleTags[tagNum]\n",
    "        start = tag[0]\n",
    "        end = tag[1]\n",
    "        charsUsedOneTag = set()\n",
    "        \n",
    "        if start == -1:\n",
    "            overlapTagNums.append(tagNum)\n",
    "            continue\n",
    "        \n",
    "        for char in range(start, end):\n",
    "            charsUsedOneTag.add(char)\n",
    "        intersect = charsUsedOverall.intersection(charsUsedOneTag)\n",
    "        if len(intersect) == 0:\n",
    "            charsUsedOverall = charsUsedOverall.union(charsUsedOneTag)\n",
    "        else:\n",
    "            overlapTagNums.append(tagNum)\n",
    "    \n",
    "    #remove overlapping tags\n",
    "    for tagNum in reversed(overlapTagNums):\n",
    "        tupleTags.pop(tagNum)\n",
    "    return tupleTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllLabels(modified_labels):\n",
    "    \"\"\"\n",
    "    A function that combines standard labels that are default to the spacy model and modified labels that the user wants to train.\n",
    "    \n",
    "    Arguments:\n",
    "    modified_labes -- a list of the modified labels in CAPS the user wants to add to the model.\n",
    "    \n",
    "    Returns:\n",
    "    all_labels -- a list of all labels for the model to be trained on.    \n",
    "    \"\"\"\n",
    "    \n",
    "    standard_labels = [\n",
    "    'PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', \n",
    "    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'\n",
    "    ]\n",
    "    \n",
    "    all_labels = all_labels = standard_labels + modified_labels\n",
    "    \n",
    "    return all_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefaultAndModifiedLabels(default_label_trainings, modified_label_trainings):\n",
    "    \"\"\"\n",
    "    A function that combines the combines the default spaCy labels and the modified labels for training texts.\n",
    "    Each training text has the modified labels (such as LAND SIZE) extracted manually.\n",
    "    But, the same text is also run through the default spaCy NER model to counteract the catastrophic forgetting\n",
    "    problem described here: https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    default_label_trainings -- a list of training texts with their default labels in the form (TEXT, {'entities': [(START_CHAR, END_CHAR, LABEL)]})\n",
    "    modified_label_trainings -- a list of training texts with their modified labels in the form (TEXT, {'entities': [(START_CHAR, END_CHAR, LABEL)]})\n",
    "    \n",
    "    Returns:\n",
    "    all_train_data -- a list containing a text with its default and modified labels with the overlaps removed in the form (TEXT, {'entities': [(START_CHAR, END_CHAR, LABEL)]})\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_train_data = []\n",
    "    for i in range(len(modified_label_trainings)):\n",
    "        per_ent_labels = modified_label_trainings[i][1]['entities'] + default_label_trainings[i][1]['entities']\n",
    "        unique_labels = removeOverlapsAndBadEntries(per_ent_labels) #to remove duplicate labels\n",
    "        an_entry = [default_label_trainings[i][0], {'entities': unique_labels}]\n",
    "        all_train_data.append(an_entry)\n",
    "        \n",
    "    return all_train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
